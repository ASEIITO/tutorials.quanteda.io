  ---
title: Japanese
weight: 30
draft: false
---

{{% author %}}By Kohei Watanabe{{% /author %}} 

We usually do not need to use external tools such as Mecab to pre-process and analyze Japanese texts, because `tokens()` can segment Japanese texts based on rules defined in the Unicode library. 

```{r, message=FALSE}
require(quanteda)
require(quanteda.corpora)
options(width = 110)
```

`data_corpus_udhr` contains the Universal Declaration of Human Rights in multiple languages including Japanese. After tokenization, we remove grammatical words using `stopwords("ja", source = "marimo")`.

```{r}
corp <- corpus_reshape(data_corpus_udhr["jpn"], to = "paragraphs")
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, padding = TRUE) %>% 
  tokens_remove(stopwords("ja", source = "marimo"), padding = TRUE)
print(toks[4:5], max_ndoc = 1, max_ntok = -1)
```

We can imporove tokenization by collocation analysis in a similar way as [compound multi-word expressions](advanced-operations/compound-mutiword-expressions/) in English texts. `"^[ァ-ヶー一-龠]+$"` is a regular expression to select only katakana and kanji. We set `padding = TRUE` to keep the distance between words.

```{r}
tstat_col <- toks %>% 
  tokens_select("^[ァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE) %>%  
  textstat_collocations()
print(tstat_col)
```

After compounding of statistically significantly associated collocations (`tstat_col$z > 3`), we can remove single-character tokens (`min_nchar = 2`) to further remove grammatical words.

```{r}
toks_comp <- tokens_compound(toks, tstat_col[tstat_col$z > 3], concatenator = "") %>% 
  tokens_select(min_nchar = 2)
print(toks_comp[4:5], max_ndoc = 1, max_ntok = -1)
```

```{r}
dfmat <- dfm(toks_comp)
print(dfmat)
```

