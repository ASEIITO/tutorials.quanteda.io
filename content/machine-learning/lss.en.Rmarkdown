---
title: Latent Semantic Scaling
weight: 70
draft: false
---

Latent Semantic Scaling (LSS) is a flexible and cost-efficient semi-supervised document scaling technique. Users only need to provide a small set of "seed words" to scale documents on a specific dimension.

```{r message=FALSE}
require(quanteda)
require(quanteda.corpora)
require(LSX)
```

```{r eval=FALSE}
corp_news <- download("data_corpus_guardian")
```

```{r include=FALSE}
# This code is only for website generation
corp_news <- readRDS("../data/data_corpus_guardian.rds")
```

We segment news articles into sentences to accurately estimate semantic proximity between words. In addition to standard pre-processing, we use the [Marimo](https://github.com/koheiw/marimo) stopwords list (`source = "marimo"`) to remove words commonly used in news articles.

```{r, cache=TRUE}
corp_sent <- corpus_reshape(corp_news, to =  "sentences")
toks_sent <- corp_sent %>% 
    tokens(remove_punct = TRUE, remove_symbols = TRUE, 
           remove_numbers = TRUE, remove_url = TRUE) %>% 
    tokens_remove(stopwords("en", source = "marimo")) %>%
    tokens_remove(c("*-time", "*-timeUpdated", "GMT", "BST", "*.com"))  
dfmat_sent <- toks_sent %>% 
    dfm(remove = "") %>% 
    dfm_trim(min_termfreq = 5)
```

```{r}
topfeatures(dfmat_sent, 20)
```

We use generic sentiment seed words to perform sentiment analysis.

```{r}
seed <- as.seedwords(data_dictionary_sentiment)
print(seed)
```

With the seed words, LSS computes polarity of words that frequently occur around ecnomy, economic etc (`pattern = "econom*"`).

```{r}
eco <- char_context(toks_sent, pattern = "econom*", p = 0.05)
tmod_lss <- textmodel_lss(dfmat_sent, seeds = seed,
                     terms = eco, k = 300, cache = TRUE)
```

```{r}
head(coef(lss), 20) # most positive words
tail(coef(lss), 20) # most negative words
```

By highlighting negative words in a manually compiled sentiment dictionary (`data_dictionary_LSD2015`), we can confirm that many but no all the words are scored negatively by LSS.

```{r words}
textplot_terms(tmod_lss, data_dictionary_LSD2015["negative"])
```

We reconstruct original documents from sentences using `dfm_group()` before predicting polarity of documents.

```{r}
dfmat_doc <- dfm_group(dfmat_sent)
dat <- docvars(dfmat_doc)
dat$fit <- predict(lss, newdata = dfmat_doc)
```

We can smooth polarity of documents to visualize the trend using `smooth_lss()`. If `engine = "locfit"`, smoothing is very fast even when we have many documents.

```{r}
dat_smooth <- smooth_lss(dat, engine = "locfit")
head(dat_smooth)
```

The circles are polarity scores of documents and the curve is their local means with 95% confidence intervals.

```{r fig.height=5, fig.width=10}
plot(dat$date, dat$fit, col = rgb(0, 0, 0, 0.05), pch = 16, ylim = c(-0.5, 0.5),
     xlab = "Time", ylab = "Economic sentiment")
lines(dat_smooth$date, dat_smooth$fit, type = "l")
lines(dat_smooth$date, dat_smooth$fit + dat_smooth$se.fit * 1.96, type = "l", lty = 3)
lines(dat_smooth$date, dat_smooth$fit - dat_smooth$se.fit * 1.96, type = "l", lty = 3)
abline(h = 0, lty = c(1, 2))
```
{{% notice ref %}}
- Watanabe, K. 2020. [Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages](https://www.tandfonline.com/doi/full/10.1080/19312458.2020.1832976). Communication Methods and Measures. {{% /notice %}}
